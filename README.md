# üìö Multimodal Pretraining Data Collection Guidelines

---

## ‚úÖ Good Starting Sources for Pretraining Data

### üéì Education & Reference Materials
- All **educational books (2000‚Äì2020)** from K‚Äì12, college, and trade schools  
  > Wide topic coverage, typically factual and non-opinionated
- All **college/university textbooks** for each programming language
- All **‚Äúshow your work‚Äù math** materials and solution guides
- **Dictionaries** for all languages
- **Translation books** (1900‚Äì2020)
- **Language learning books** for:
  - Programming
  - Math
  - English (and equivalents in each local language)

### üìö Technical & Academic
- All **arXiv papers**
- All **patents** (USPTO, WIPO, etc.)
- **High-quality codebases** that:
  - Compile successfully
  - Have no known vulnerabilities
  - Represent a variety of programming languages
- **College-level final papers** and long-form student essays
- **Game rulebooks**: board, card, yard, and pool games

### üç≥ Knowledge & Culture
- **Cookbooks and recipes** (1800‚Äì2020)
- **Educational YouTube videos**, especially:
  - Verified sources (e.g., Khan Academy, CrashCourse)
  - Accurate **closed captions** only (no autogenerated subs)
- **National Geographic YouTube videos** (closed-captioned)

### üóûÔ∏è News
- Articles from **multiple verified news sources**
- ‚ùó **Exception for AI-generated content**:  
  > Use multi-source summaries to create **bias-exposing rewrites** ‚Äî clearly flagged in metadata.

---

## ‚ö†Ô∏è Important Constraints

### üñºÔ∏è Allowed File Types for Multimodal Inputs
- **Images**: `.png`, `.jpg`, `.webp`
- **Audio**: `.wav`, `.mp3`
- **Video**: `.mp4`, `.webm`, `.avi`, `.mov`

### üìù Metadata Logging (Required Per Sample)
```json
{
  "source": "URL or archive name",
  "uploader": "username/channel/organization",
  "platform": "e.g., arXiv, YouTube, Project Gutenberg",
  "timestamp": "YYYY-MM-DD",
  "format": "filetype or encoding",
  "filtering_notes": "e.g., speculative, political, verified"
}
```

---

## üö´ Do Not Include the Following

1. ‚ùå **Speculative Content**
   - No predictions or guesses about the **future**
   - Avoid conditional phrasing about uncertain events

2. ‚ùå **Political Commentary or Partisan Framing**
   - Do not include political opinions or ideology
   - Use neutral terms like **‚Äúthe government‚Äù** instead of specific political entities

3. ‚ùå **AI-Generated or Synthetic Content**
   - Do not include content that is known or suspected to be generated by AI:
     - Code
     - News
     - Images
     - Audio
     - Video  
   - ‚úÖ **Only exception**: bias-injected news summaries from multiple human sources (clearly marked)
   
4. ‚ùå **Real People**
   - Do not include real people information like:
     - Phone number
     - Address
     - Full Name
     - SSN
     - Employment
    - Instead use a person from our fake peolpe folder to use. This will limit PII

---

## Setting for the tokenizer

FINAL_VOCAB_SIZE = 100000

SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""

TOKENIZER_CONFIG_CONTENT = {
    "bos_token": "<|begin_of_text|>",
    "eos_token": "<|eot|>",
    "pad_token": "<|finetune_right_pad|>",
    "clean_up_tokenization_spaces": False,
    "model_input_names": ["input_ids", "attention_mask"],
    "chat_template": "..."  # Use the full chat template string you provided earlier here.
}

# Special tokens
special_tokens = [
    '<|begin_of_text|>', '<|end_of_text|>', '<|fim_prefix|>', '<|fim_middle|>', '<|fim_suffix|>',
    '<|header_start|>', '<|header_end|>', '<|eom|>', '<|eot|>', '<|step|>',
    '<|finetune_right_pad|>', '<|finetune_left_pad|>', '<|unk|>',
    '<think>', '</think>', '<tool_call>', '</tool_call>', '<tool_response>', '</tool_response>',
    '<code>', '</code>', '<|audio_start|>', '<|audio_end|>', '<|audio|>', '<|wave|>', '<|spectro|>',
    '<|sep_img|>', '<|tile_x_sep|>', '<|tile_y_sep|>', '<|vision_start|>', '<|vision_end|>', '<|image|>', '<|patch|>'
]
special_tokens += [f"<|reserved_token_{i}|>" for i in range(25)]


tokenizer = Tokenizer(models.BPE(unk_token="<|unk|>"))
tokenizer.pre_tokenizer = pre_tokenizers.Regex(SPLIT_PATTERN)
trainer = trainers.BpeTrainer(vocab_size=FINAL_VOCAB_SIZE, special_tokens=special_tokens)

# Post-processing (optional)
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"<|begin_of_text|> $A <|eot|>",
    pair=f"<|begin_of_text|> $A <|eot|> $B:1 <|eot|>:1",
    special_tokens=[
        ("<|begin_of_text|>", tokenizer.token_to_id("<|begin_of_text|>")),
        ("<|eot|>", tokenizer.token_to_id("<|eot|>")),
    ],
)

---

## üõ†Ô∏è Optional Enhancements

- **Language coverage index**: Ensure multilingual representation
- **Accessibility support**: Favor content with closed captions or alt-text
- **Redundancy flag**: Detect and remove duplicate samples

